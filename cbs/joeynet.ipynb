{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import ctraining_data\n",
    "\n",
    "# basic RESNET style convolution block, \n",
    "# these blocks make up the bulk of the network\n",
    "class SimpleResidualConv2D(nn.Module):\n",
    "    def __init__(self, channels: int, H: int, W: int, kernel_size: int):\n",
    "        super().__init__()\n",
    "        self.params = nn.ParameterList()\n",
    "        pad = int((kernel_size-1)//2)\n",
    "        self.conv1 = nn.LazyConv2d(channels, kernel_size, padding=pad, groups=1)\n",
    "        self.params.extend(self.conv1.parameters())\n",
    "        self.bn1 = nn.LazyBatchNorm2d()\n",
    "        self.params.extend(self.bn1.parameters())\n",
    "        self.conv2 = nn.LazyConv2d(channels, kernel_size, padding=pad, groups=1)\n",
    "        self.params.extend(self.conv2.parameters())\n",
    "        self.bn2 = nn.LazyBatchNorm2d()\n",
    "        self.params.extend(self.bn2.parameters())\n",
    "    \n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        return F.relu(Y)+X\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.params\n",
    "\n",
    "# The \"JoeyBlock\" chains a SimpleResidualConv2D unit with a 1x1 convolution, merely to \n",
    "# introduce an extra layer of full-connections between channels.\n",
    "# This might be over-kill\n",
    "class JoeyBlock(nn.Module):\n",
    "    def __init__(self, channels_in, \n",
    "                 channels_out, \n",
    "                 Hin, Win, \n",
    "                 ckernel_size):\n",
    "        super().__init__()\n",
    "        self.params = nn.ParameterList()\n",
    "        self.channels_in = channels_in\n",
    "        self.channels_out = channels_out\n",
    "        self.res = SimpleResidualConv2D(channels_in, Hin, Win, ckernel_size)\n",
    "        self.params.extend(self.res.parameters())\n",
    "        self.conv1x1 = nn.Conv2d(channels_in, channels_out, 1)\n",
    "        self.params.extend(self.conv1x1.parameters())\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return F.relu(self.conv1x1(self.res(X)))\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.params\n",
    "\n",
    "# \"JoeyNet\" consists of a series of \"JoeyBlocks\", followed by a head stage\n",
    "# consisting of 3 fully-connected layers that consume the flattened convolution channels.\n",
    "class JoeyNet(nn.Module):\n",
    "    def __init__(self, channels_in, Hin, Win, num_blocks, ckernel_size):\n",
    "        super().__init__()\n",
    "        self.params = nn.ParameterList()\n",
    "        features_out = channels_in-1\n",
    "        self.blocks = [] \n",
    "        for i in range(num_blocks):\n",
    "            block = JoeyBlock(channels_in, channels_in, Hin, Win, ckernel_size)\n",
    "            self.blocks.append(block)\n",
    "            self.params.extend(block.parameters())\n",
    "        \n",
    "        self.fc = []\n",
    "        for i in range(1,4):\n",
    "            fc = nn.LazyLinear(features_out)\n",
    "            self.fc.append(fc)\n",
    "            self.params.extend(fc.parameters())\n",
    "    \n",
    "    \"\"\"\n",
    "    Inputs to the forward pass of the network\n",
    "    maps: a (N+1, H, W) tensor.\n",
    "        - the first channel is a map of the empty environment\n",
    "        - the remaining channels encode A* shortest paths for single agents that start\n",
    "            on the boundary, and merely the goal location of all other agents. Hence,\n",
    "            there is uncertainty built into the model.\n",
    "\n",
    "    astar_delays: a (N,) tensor containing the length of the A* shortest paths for\n",
    "        each agent that starts on the boundary. This is added to the output of\n",
    "        the network, so that in effect, the network learns a \"correction\" to path-lengths\n",
    "        predicted by A*.\n",
    "    \n",
    "    output_mask: a (N,) tensor of 1's and 0's, used to mask (zero) the output channels of \n",
    "        network which are associated with agents who's initial position in the environment\n",
    "        is not known, but whos goal positions are known.\n",
    "    \"\"\"\n",
    "    def forward(self, maps, astar_delays, output_mask):\n",
    "        Y = maps\n",
    "        for i in range(len(self.blocks)):\n",
    "            Y = self.blocks[i](Y)\n",
    "        Y = torch.flatten(Y, start_dim=1)\n",
    "        for i in range(len(self.fc)-1):\n",
    "            Y = F.relu(self.fc[i](Y))\n",
    "        Y = self.fc[-1](Y)\n",
    "        return output_mask*(astar_delays + Y)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.params\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import the training dataset, and construct a data loader.\n",
    "\"\"\"\n",
    "import ctraining_data\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "dataset = ctraining_data.ColumnLatticeDataset('data')\n",
    "batch_size = 500\n",
    "\n",
    "# note that multiple processes can be assigned in this constructor,\n",
    "# so that loading of large batches is sped up (this is a good idea when doing GPU training)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joseph\\Documents\\multirobot_planning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 6.966172695159912\n",
      "Epoch loss: 5.768726348876953\n",
      "Epoch loss: 4.663089752197266\n",
      "Epoch loss: 3.6063694953918457\n",
      "Epoch loss: 2.856055974960327\n",
      "Epoch loss: 2.4778196811676025\n",
      "Epoch loss: 2.50622820854187\n",
      "Epoch loss: 2.914788246154785\n",
      "Epoch loss: 3.449183702468872\n",
      "Epoch loss: 3.6854705810546875\n",
      "Epoch loss: 3.4314069747924805\n",
      "Epoch loss: 2.901149272918701\n",
      "Epoch loss: 2.3619682788848877\n",
      "Epoch loss: 1.9962055683135986\n",
      "Epoch loss: 1.8531579971313477\n",
      "Epoch loss: 1.8933385610580444\n",
      "Epoch loss: 2.040192127227783\n",
      "Epoch loss: 2.2066726684570312\n",
      "Epoch loss: 2.2698140144348145\n",
      "Epoch loss: 2.1687710285186768\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Construct the network. In this test, there are N=13 agents maximum\n",
    "in the environment at any given time. This results in N+1=14 input channels.\n",
    "\n",
    "I used 10 \"JoeyBlocks\" to construct the body of the network. They arent massive, but \n",
    "it may be overlarge. I'm not sure.\n",
    "\n",
    "Inside the \"JoeyBlocks\" are convolution channels, the kernel size is set by ckernel_size.\n",
    "These blocks, due to their residual nature, include padding to maintain the channel dimensions.\n",
    "\"\"\"\n",
    "channels_in = 14\n",
    "Hin = 14\n",
    "Win = 14\n",
    "num_blocks = 10\n",
    "ckernel_size = 5\n",
    "model = JoeyNet(channels_in, Hin, Win, num_blocks, ckernel_size)\n",
    "\n",
    "# run a forward pass with gradients disabled to initialize lazy modules\n",
    "features, labels = next(iter(dataloader))\n",
    "with torch.no_grad():\n",
    "    model(*features)\n",
    "\n",
    "# Using ADAM as our optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Sum of squared errors as loss function\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Training for 20 epochs\n",
    "epochs = 20\n",
    "epoch_losses = []\n",
    "for i in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for features, labels in iter(dataloader):\n",
    "        Y = model(*features)\n",
    "        loss = loss_fn(Y,labels)\n",
    "        epoch_loss += loss \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_losses.append(epoch_loss/len(dataset))\n",
    "    print(f\"Epoch loss: {epoch_loss/len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-3114.)\n",
      "tensor(-3040.)\n",
      "tensor(-3080.)\n",
      "tensor(-3134.)\n",
      "tensor(-157.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3528.7036)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparing just A* solutions to the CBS solutions\n",
    "astar_losses = torch.zeros(len(dataset), dtype=torch.float)\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    for features, labels in iter(dataloader):\n",
    "        N_batch = labels.size()[0]\n",
    "        astar_losses[i:i+N_batch] = loss_fn(features[1], labels)\n",
    "        i += N_batch\n",
    "astar_losses.sum()/len(dataset) # about 1000x worse than running the congestion model on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3582.)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "astar_losses[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025\n"
     ]
    }
   ],
   "source": [
    "i = iter(dataloader)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([ 4.1784, 13.7605, 10.7260,  6.5246, 22.0122, 17.6655, -0.0000,  0.0000,\n",
      "         0.0000, -0.0000, -0.0000, -0.0000, -0.0000], grad_fn=<SliceBackward0>)\n",
      "Actual: tensor([ 4., 14., 11.,  7., 22., 18.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n"
     ]
    }
   ],
   "source": [
    "features,labels = next(i)\n",
    "print(f'Predicted: {model(*features)[1,:]}')\n",
    "print(f'Actual: {labels[1,:]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cbs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
